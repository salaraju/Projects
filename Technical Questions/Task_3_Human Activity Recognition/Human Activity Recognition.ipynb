{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH2etcDLZ47n"
      },
      "source": [
        "### Human activity Video Classifier Using CNN and RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "i am using small video's dataset of yoga, exercise, dancing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BD2hx2L1B2y0"
      },
      "outputs": [],
      "source": [
        "#import very useful python libraries\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "dataset_path = \"/content/dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqWXauHeDftv",
        "outputId": "582b0c89-021d-4fdf-8b17-cf228656d834"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/datasettrain.csv', '/content/datasettest.csv']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filenames = [dataset_path + filename for filename in os.listdir(dataset_path)]\n",
        "filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxhbm93vJJwe",
        "outputId": "9c8c50dc-37ce-47a3-d504-33f58ff35ee7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/dataset\n"
          ]
        }
      ],
      "source": [
        "cd /content/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "UChqiPkVJLQy",
        "outputId": "1918a4b7-66ce-4952-c05b-77d71204fd2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total videos for training: 145\n",
            "Total videos for testing: 22\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-df031029-9101-4646-bd6a-a0d52a28a4b2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>80</td>\n",
              "      <td>dataset/train/exercise/exercise (4).mp4</td>\n",
              "      <td>exercise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>65</td>\n",
              "      <td>dataset/train/exercise/exercise (12).mp4</td>\n",
              "      <td>exercise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>115</td>\n",
              "      <td>dataset/train/yoga/yoga (5).mp4</td>\n",
              "      <td>yoga</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>93</td>\n",
              "      <td>dataset/train/exercise/exercisee (6).mp4</td>\n",
              "      <td>exercise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>51</td>\n",
              "      <td>dataset/train/exercise/exercis (10).mp4</td>\n",
              "      <td>exercise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>50</td>\n",
              "      <td>dataset/train/exercise/exercis (1).mp4</td>\n",
              "      <td>exercise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>141</td>\n",
              "      <td>dataset/train/yoga/yoga_asana (6).mp4</td>\n",
              "      <td>yoga</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>47</td>\n",
              "      <td>dataset/train/dancing/dancings (7).mp4</td>\n",
              "      <td>dancing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>70</td>\n",
              "      <td>dataset/train/exercise/exercise (17).mp4</td>\n",
              "      <td>exercise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>64</td>\n",
              "      <td>dataset/train/exercise/exercise (11).mp4</td>\n",
              "      <td>exercise</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df031029-9101-4646-bd6a-a0d52a28a4b2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-df031029-9101-4646-bd6a-a0d52a28a4b2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-df031029-9101-4646-bd6a-a0d52a28a4b2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Unnamed: 0                                video_name       tag\n",
              "80           80   dataset/train/exercise/exercise (4).mp4  exercise\n",
              "65           65  dataset/train/exercise/exercise (12).mp4  exercise\n",
              "115         115           dataset/train/yoga/yoga (5).mp4      yoga\n",
              "93           93  dataset/train/exercise/exercisee (6).mp4  exercise\n",
              "51           51   dataset/train/exercise/exercis (10).mp4  exercise\n",
              "50           50    dataset/train/exercise/exercis (1).mp4  exercise\n",
              "141         141     dataset/train/yoga/yoga_asana (6).mp4      yoga\n",
              "47           47    dataset/train/dancing/dancings (7).mp4   dancing\n",
              "70           70  dataset/train/exercise/exercise (17).mp4  exercise\n",
              "64           64  dataset/train/exercise/exercise (11).mp4  exercise"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "\n",
        "train_df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7x6Cek9KzID",
        "outputId": "69179efa-bdd7-4c55-a85c-607aa7b8472c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/tensorflow/docs\n",
            "  Cloning https://github.com/tensorflow/docs to /tmp/pip-req-build-ppxwelyh\n",
            "  Running command git clone -q https://github.com/tensorflow/docs /tmp/pip-req-build-ppxwelyh\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (0.8.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (1.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (2.11.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (3.17.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (3.13)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<3.20,>=3.12.0->tensorflow-docs==0.0.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->tensorflow-docs==0.0.0.dev0) (2.0.1)\n",
            "Building wheels for collected packages: tensorflow-docs\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-docs: filename=tensorflow_docs-0.0.0.dev0-py3-none-any.whl size=180247 sha256=519b54306b74e23e6d0ce76ec4340201fe6f61e02d56a1cc03ae8b579f61465b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ghgq5ecb/wheels/cc/c4/d8/5341e93b6376c5c929c49469fce21155eb69cef1a4da4ce32c\n",
            "Successfully built tensorflow-docs\n",
            "Installing collected packages: tensorflow-docs\n",
            "Successfully installed tensorflow-docs-0.0.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/tensorflow/docs\n",
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow import keras\n",
        "from imutils import paths\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SQRwahCnQAXJ"
      },
      "outputs": [],
      "source": [
        "# The following two methods are taken from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "IMG_SIZE = 224\n",
        "\n",
        "\n",
        "def crop_center_square(frame):\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
        "\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center_square(frame)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojj8DHESQx52",
        "outputId": "3d8ab141-9839-4843-9b26-92f1a935fd0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "87924736/87910968 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "def build_feature_extractor():\n",
        "    feature_extractor = keras.applications.InceptionV3(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6eL2Vc2Q5EX",
        "outputId": "be1b2acb-c282-4426-9b4c-2e570de23c32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['dancing', 'exercise', 'yoga']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2]])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "labels = train_df[\"tag\"].values\n",
        "labels = label_processor(labels[..., None]).numpy()\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "W2YKWNlhRD1t"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "\n",
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p-ItYrERFds",
        "outputId": "cfe89004-4813-4343-9e85-b7e9af7c4ee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frame features in train set: (145, 20, 2048)\n",
            "Frame masks in train set: (145, 20)\n",
            "train_labels in train set: (145, 1)\n",
            "test_labels in train set: (22, 1)\n"
          ]
        }
      ],
      "source": [
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    \n",
        "    ##take all classlabels from train_df column named 'tag' and store in labels\n",
        "    labels = df[\"tag\"].values\n",
        "    \n",
        "    #convert classlabels to label encoding\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
        "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
        "    # masked with padding or not.\n",
        "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\") # 145,20\n",
        "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\") #145,20,2048\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholders to store the masks and features of the current video.\n",
        "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                    batch[None, j, :]\n",
        "                )\n",
        "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
        "\n",
        "    return (frame_features, frame_masks), labels\n",
        "\n",
        "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
        "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
        "print(f\"Frame masks in train set: {train_data[1].shape}\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"train_labels in train set: {train_labels.shape}\")\n",
        "\n",
        "print(f\"test_labels in train set: {test_labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6KDzKnmRFar",
        "outputId": "979fb2af-8b5f-47d2-d772-72e7ee160c26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0980 - accuracy: 0.4257\n",
            "Epoch 1: val_loss improved from inf to 1.10356, saving model to ./tmp/video_classifier\n",
            "4/4 [==============================] - 12s 825ms/step - loss: 1.0980 - accuracy: 0.4257 - val_loss: 1.1036 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0959 - accuracy: 0.4950\n",
            "Epoch 2: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 113ms/step - loss: 1.0959 - accuracy: 0.4950 - val_loss: 1.1082 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0939 - accuracy: 0.4950\n",
            "Epoch 3: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 118ms/step - loss: 1.0939 - accuracy: 0.4950 - val_loss: 1.1131 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0917 - accuracy: 0.4950\n",
            "Epoch 4: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 129ms/step - loss: 1.0917 - accuracy: 0.4950 - val_loss: 1.1179 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0896 - accuracy: 0.4950\n",
            "Epoch 5: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 118ms/step - loss: 1.0896 - accuracy: 0.4950 - val_loss: 1.1226 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0877 - accuracy: 0.4950\n",
            "Epoch 6: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 128ms/step - loss: 1.0877 - accuracy: 0.4950 - val_loss: 1.1272 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0858 - accuracy: 0.4950\n",
            "Epoch 7: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 128ms/step - loss: 1.0858 - accuracy: 0.4950 - val_loss: 1.1317 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0838 - accuracy: 0.4950\n",
            "Epoch 8: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 120ms/step - loss: 1.0838 - accuracy: 0.4950 - val_loss: 1.1362 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0819 - accuracy: 0.4950\n",
            "Epoch 9: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 128ms/step - loss: 1.0819 - accuracy: 0.4950 - val_loss: 1.1408 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0801 - accuracy: 0.4950\n",
            "Epoch 10: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 134ms/step - loss: 1.0801 - accuracy: 0.4950 - val_loss: 1.1454 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0782 - accuracy: 0.4950\n",
            "Epoch 11: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 131ms/step - loss: 1.0782 - accuracy: 0.4950 - val_loss: 1.1499 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0763 - accuracy: 0.4950\n",
            "Epoch 12: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 131ms/step - loss: 1.0763 - accuracy: 0.4950 - val_loss: 1.1545 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0745 - accuracy: 0.4950\n",
            "Epoch 13: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 115ms/step - loss: 1.0745 - accuracy: 0.4950 - val_loss: 1.1591 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0726 - accuracy: 0.4950\n",
            "Epoch 14: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 123ms/step - loss: 1.0726 - accuracy: 0.4950 - val_loss: 1.1638 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0708 - accuracy: 0.4950\n",
            "Epoch 15: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 129ms/step - loss: 1.0708 - accuracy: 0.4950 - val_loss: 1.1686 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0689 - accuracy: 0.4950\n",
            "Epoch 16: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 134ms/step - loss: 1.0689 - accuracy: 0.4950 - val_loss: 1.1733 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0671 - accuracy: 0.4950\n",
            "Epoch 17: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 115ms/step - loss: 1.0671 - accuracy: 0.4950 - val_loss: 1.1779 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0653 - accuracy: 0.4950\n",
            "Epoch 18: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 129ms/step - loss: 1.0653 - accuracy: 0.4950 - val_loss: 1.1825 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0636 - accuracy: 0.4950\n",
            "Epoch 19: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 126ms/step - loss: 1.0636 - accuracy: 0.4950 - val_loss: 1.1869 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0618 - accuracy: 0.4950\n",
            "Epoch 20: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 119ms/step - loss: 1.0618 - accuracy: 0.4950 - val_loss: 1.1913 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0602 - accuracy: 0.4950\n",
            "Epoch 21: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 113ms/step - loss: 1.0602 - accuracy: 0.4950 - val_loss: 1.1959 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0584 - accuracy: 0.4950\n",
            "Epoch 22: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 122ms/step - loss: 1.0584 - accuracy: 0.4950 - val_loss: 1.2005 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0567 - accuracy: 0.4950\n",
            "Epoch 23: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 117ms/step - loss: 1.0567 - accuracy: 0.4950 - val_loss: 1.2053 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0550 - accuracy: 0.4950\n",
            "Epoch 24: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 120ms/step - loss: 1.0550 - accuracy: 0.4950 - val_loss: 1.2101 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0531 - accuracy: 0.4950\n",
            "Epoch 25: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 120ms/step - loss: 1.0531 - accuracy: 0.4950 - val_loss: 1.2148 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0514 - accuracy: 0.4950\n",
            "Epoch 26: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 129ms/step - loss: 1.0514 - accuracy: 0.4950 - val_loss: 1.2196 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0497 - accuracy: 0.4950\n",
            "Epoch 27: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 125ms/step - loss: 1.0497 - accuracy: 0.4950 - val_loss: 1.2245 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0479 - accuracy: 0.4950\n",
            "Epoch 28: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 126ms/step - loss: 1.0479 - accuracy: 0.4950 - val_loss: 1.2293 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0462 - accuracy: 0.4950\n",
            "Epoch 29: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 122ms/step - loss: 1.0462 - accuracy: 0.4950 - val_loss: 1.2342 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - ETA: 0s - loss: 1.0445 - accuracy: 0.4950\n",
            "Epoch 30: val_loss did not improve from 1.10356\n",
            "4/4 [==============================] - 0s 129ms/step - loss: 1.0445 - accuracy: 0.4950 - val_loss: 1.2390 - val_accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0992 - accuracy: 0.2273\n",
            "Test accuracy: 22.73%\n"
          ]
        }
      ],
      "source": [
        "# Utility for our sequence model.\n",
        "def get_sequence_model():\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
        "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "\n",
        "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
        "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
        "    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)\n",
        "    x = keras.layers.GRU(8)(x)\n",
        "    x = keras.layers.Dropout(0.4)(x)\n",
        "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
        "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
        "\n",
        "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
        "\n",
        "    rnn_model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return rnn_model\n",
        "\n",
        "EPOCHS = 30\n",
        "# Utility for running experiments.\n",
        "def run_experiment():\n",
        "    filepath = \"./tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    seq_model = get_sequence_model()\n",
        "    history = seq_model.fit(\n",
        "        [train_data[0], train_data[1]],\n",
        "        train_labels,\n",
        "        validation_split=0.3,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    seq_model.load_weights(filepath)\n",
        "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history, seq_model\n",
        "\n",
        "\n",
        "_, sequence_model = run_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7fjISCGRFX6",
        "outputId": "a14074dd-e493-4058-e018-6e01bf1cb699"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test video path: dataset/test/yoga/yoga (24).mp4\n",
            "  dancing: 33.42%\n",
            "  exercise: 33.41%\n",
            "  yoga: 33.17%\n"
          ]
        }
      ],
      "source": [
        "def prepare_single_video(frames):\n",
        "    frames = frames[None, ...]\n",
        "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
        "\n",
        "    for i, batch in enumerate(frames):\n",
        "        video_length = batch.shape[0]\n",
        "        length = min(MAX_SEQ_LENGTH, video_length)\n",
        "        for j in range(length):\n",
        "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
        "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "    return frame_features, frame_mask\n",
        "\n",
        "\n",
        "def sequence_prediction(path):\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frames = load_video(os.path.join(\"test\", path))\n",
        "    frame_features, frame_mask = prepare_single_video(frames)\n",
        "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
        "\n",
        "    for i in np.argsort(probabilities)[::-1]:\n",
        "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
        "    return frames\n",
        "\n",
        "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
        "print(f\"Test video path: {test_video}\")\n",
        "\n",
        "test_frames = sequence_prediction(test_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw3GkGpHRFVc"
      },
      "source": [
        "### this accuracy is low. because the train & test dataset is very small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxvRzsDrRFS_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVJKO4U5RFQt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfNLG7NCRFOE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "my_work.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
